{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Webscraping Project 4 Lab\n",
    "\n",
    "Week 4 | Day 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "In this project, we will practice two major skills: collecting data by scraping a website and then building a binary predictor with Logistic Regression.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title and summary of the job we will attempt to predict the salary of the job. For job posting sites, this would be extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries from other listings can help guide negotiations.\n",
    "\n",
    "Normally, we could use regression for this task; however, we will convert this problem into classification and use Logistic Regression.\n",
    "\n",
    "- Question: Why would we want this to be a classification problem?\n",
    "- Answer: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Therefore, the first part of the assignment will be focused on scraping Indeed.com (or other sites at your team's discretion). In the second part, the focus is on using listings with salary information to build a model and predict high or low salaries and what features are predictive of that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "We will be scraping job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries.\n",
    "\n",
    "First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\")\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9732c901-ae26-4160-8376-42e22dd327df"
   },
   "source": [
    "#### Setup a request (using `requests`) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:49:40.859527",
     "start_time": "2016-10-17T17:49:39.121815"
    },
    "collapsed": true,
    "focus": false,
    "id": "2efefc73-064a-482d-b3b5-ddf5508cb4ec"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:29:49.642381",
     "start_time": "2016-10-17T12:29:44.341111"
    },
    "collapsed": false,
    "focus": false,
    "id": "2c6752c4-7704-4c94-8bc0-6f13d2d0d570"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium import webdriver\n",
    "\n",
    "dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path='/Applications/anaconda/anaconda/bin/phantomJS', desired_capabilities=dcap)\n",
    "driver.set_window_size(1024, 768) \n",
    "\n",
    "URL = \"http://www.indeed.com/jobs?q=data+scientist&l=New+York&start=20\"\n",
    "\n",
    "driver.get(URL)\n",
    "page_html = driver.page_source\n",
    "soup = BeautifulSoup(page_html, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "bb0b866a-26a7-45e9-8084-5a0f90eb4b3e"
   },
   "source": [
    "Let's look at one result more closely. A single `result` looks like\n",
    "\n",
    "```\n",
    "<div class=\" row result\" data-jk=\"2480d203f7e97210\" data-tn-component=\"organicJob\" id=\"p_2480d203f7e97210\" itemscope=\"\" itemtype=\"http://schema.org/JobPosting\">\n",
    "<h2 class=\"jobtitle\" id=\"jl_2480d203f7e97210\">\n",
    "<a class=\"turnstileLink\" data-tn-element=\"jobTitle\" onmousedown=\"return rclk(this,jobmap[0],1);\" rel=\"nofollow\" target=\"_blank\" title=\"AVP/Quantitative Analyst\">AVP/Quantitative Analyst</a>\n",
    "</h2>\n",
    "<span class=\"company\" itemprop=\"hiringOrganization\" itemtype=\"http://schema.org/Organization\">\n",
    "<span itemprop=\"name\">\n",
    "<a href=\"/cmp/Alliancebernstein?from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=2480d203f7e97210&amp;jcid=b374f2a780e04789\" target=\"_blank\">\n",
    "    AllianceBernstein</a></span>\n",
    "</span>\n",
    "<tr>\n",
    "<td class=\"snip\">\n",
    "<nobr>$117,500 - $127,500 a year</nobr>\n",
    "<div>\n",
    "<span class=\"summary\" itemprop=\"description\">\n",
    "C onduct quantitative and statistical research as well as portfolio management for various investment portfolios. Collaborate with Quantitative Analysts and</span>\n",
    "</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "```\n",
    "\n",
    "While this has some more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The salary is available in a `nobr` element inside of a `td` element with `class='snip`.\n",
    "- The title of a job is in a link with class set to `jobtitle` and a `data-tn-element=\"jobTitle`.  \n",
    "- The location is set in a `span` with `class='location'`. \n",
    "- The company is set in a `span` with `class='company'`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "f1eddb90-4ba8-483c-a229-77e93aa53119"
   },
   "source": [
    "### Write 4 functions to extract each item: location, company, job, and salary.\n",
    "\n",
    "example: \n",
    "```python\n",
    "def extract_location_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "\n",
    "- Make sure these functions are robust and can handle cases where the data/field may not be available.\n",
    "- Test the functions on the results above\n",
    "- Include any other features you may want to use later (e.g. summary, #of reviews...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34070e89-9521-4b45-90c8-57a6599aac68"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results, the `l=New+York` and the `start=10`. The first controls the location of the results (so we can try a different city). The second controls where in the results to start and gives 10 results (thus, we can keep incrementing by 10 to go further in the list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e8beed7c-3e42-40c0-810f-5f67f8f885a0"
   },
   "source": [
    "### Complete the following code to collect results from multiple cities and starting points. \n",
    "- Indeed.com only has salary information for an estimated 20% of job postings. You may want to add other cities to the list below to gather more data. \n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T00:33:31.710943",
     "start_time": "2016-10-17T00:05:15.512276"
    },
    "collapsed": false,
    "focus": false,
    "id": "04b0f9af-540e-402f-8292-81748707c676"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "############### EDIT THESE CONSTANTS\n",
    "############### \n",
    "###############        EDIT THESE CONSTANTS\n",
    "\n",
    "MAX_RESULTS_PER_CITY = 1000      ### DO NOT SET MORE THAN 1000\n",
    "URL_SEARCH_TERM = 'Data Scientist' ### DO NOT SET MORE THAN SINGLE SEARCH TERM (TITLE)\n",
    "CITY_SET = ['New York', 'Chicago', 'San Francisco', 'Austin', 'Atlanta', '', 'Boston', 'Seattle'\\\n",
    "            'Los Angeles','Washington, DC', 'San Jose','Denver', 'Atlanta','Houston',\\\n",
    "            'Dallas','Nashville','San Diego','Cleveland','Minneapolis','Baltimore','Philadelphia','Detroit']\n",
    "PHANTOM_PATH = '/Applications/anaconda/anaconda/bin/phantomJS'\n",
    "###############\n",
    "################################################################\n",
    "\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import urllib\n",
    "\n",
    "def extract_location_from_resultRow(result):\n",
    "    try:\n",
    "        location = (result.find(class_='location').text.strip())\n",
    "    except:\n",
    "        location = ''\n",
    "    return location\n",
    "\n",
    "def extract_company_from_resultRow(result):\n",
    "    try:\n",
    "        company = (result.find(class_='company').text.strip())\n",
    "    except:\n",
    "        company = ''\n",
    "    return company\n",
    "\n",
    "def extract_jkid_from_resultRow(result):\n",
    "    try:\n",
    "        row = (result.find(class_='jobtitle turnstileLink'))\n",
    "        jkid = result['data-jk']\n",
    "    except: \n",
    "        jkid = ''\n",
    "    return jkid\n",
    "\n",
    "def extract_title_from_resultRow(result):\n",
    "    try:\n",
    "        title = (result.find(class_='turnstileLink'))\n",
    "        title_text = title.text\n",
    "    except: \n",
    "        title_text = ''\n",
    "    return title_text\n",
    "\n",
    "def extract_salary_from_resultRow(result):\n",
    "    try:\n",
    "        salary = (result.find(class_='snip').find('nobr').text)\n",
    "    except:\n",
    "        salary = ''\n",
    "    salary_text = salary\n",
    "    return salary_text\n",
    "\n",
    "def extract_reviews_from_resultRow(result):\n",
    "    try:\n",
    "        reviews = (result.find(class_='slNoUnderline').text.strip().strip(' reviews').replace(',',''))\n",
    "    except: \n",
    "        reviews = ''\n",
    "    return reviews\n",
    "\n",
    "def extract_stars_from_resultRow(result):\n",
    "    try: \n",
    "        stars = (result.find(class_='rating')['style']).split(';background-position:')[1].split(':')[1].split('px')[0].strip()\n",
    "    except: dro\n",
    "        stars = ''\n",
    "    return stars\n",
    "\n",
    "def extract_date_from_resultRow(result):\n",
    "    try: \n",
    "        date = (result.find(class_='date').text.strip(' ago').strip())\n",
    "    except: \n",
    "        date = ''\n",
    "    return date\n",
    "\n",
    "def extract_summary_from_resultRow(result):\n",
    "    try: \n",
    "        summary = (result.find(\"span\", {\"itemprop\" : \"description\"}).text.strip())\n",
    "    except: \n",
    "        summary = ''\n",
    "    return summary\n",
    "\n",
    "dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "dcap[\"phantomjs.page.settings.userAgent\"] = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path=PHANTOM_PATH, desired_capabilities=dcap)\n",
    "driver.set_window_size(1024, 768) \n",
    "\n",
    "for city in CITY_SET:\n",
    "    job_dict = []\n",
    "    now = datetime.datetime.now()\n",
    "    for start in range(0, MAX_RESULTS_PER_CITY, 10):\n",
    "\n",
    "        URL = \"http://www.indeed.com/jobs?q=\"+urllib.quote(URL_SEARCH_TERM)+\"&l=\"+urllib.quote(city)+\"&start=\"+str(start)\n",
    "        driver.get(URL)\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        for i in soup.findAll(\"div\", {\"data-tn-component\" : \"organicJob\"}):\n",
    "\n",
    "            location = extract_location_from_resultRow(i)\n",
    "            company = extract_company_from_resultRow(i)\n",
    "            summary = extract_summary_from_resultRow(i)\n",
    "            jkid = extract_jkid_from_resultRow(i)\n",
    "            title = extract_title_from_resultRow(i)\n",
    "            salary = extract_salary_from_resultRow(i)\n",
    "            reviews = extract_reviews_from_resultRow(i)\n",
    "            stars = extract_stars_from_resultRow(i)\n",
    "            post_date = extract_date_from_resultRow(i)\n",
    "\n",
    "            job_dict.append([location, company, summary, jkid, title, salary, stars, reviews, post_date, now])\n",
    "            \n",
    "        job_df = pd.DataFrame(job_dict, columns=['location', 'company', 'summary', 'jkid', 'title', 'salary', 'stars', 'reviews', 'post_date', 'pull_date'])       \n",
    "\n",
    "    job_df.to_csv('scrape'+city+'_'+str(MAX_RESULTS_PER_CITY)+'.csv', encoding='utf-8')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "20339c09-5032-4e27-91be-286e9b46cd13"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:43.459853",
     "start_time": "2016-10-17T17:59:43.311630"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('master.csv')\n",
    "\n",
    "# DELETE ANY HEADER ROWS LEFT OVER FROM CSV MERGE\n",
    "try: master_df = master_df[master_df['reviews'] != 'reviews'] \n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:44.320857",
     "start_time": "2016-10-17T17:59:44.069693"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### REVIEWS CLEAN TO FLOAT\n",
    "######\n",
    "\n",
    "master_df['reviews'] = master_df['reviews'].fillna(0)\n",
    "\n",
    "def indeed_review_cleanup(review): \n",
    "    try:\n",
    "        review = review.str.replace(',','')\n",
    "        review = review.strip(' reviews')\n",
    "        review = review.strip(' review')\n",
    "        review = review.strip('reviews')\n",
    "        review = review.strip()\n",
    "        review = float(review)\n",
    "    except:\n",
    "        #print review\n",
    "        pass\n",
    "    return review\n",
    "\n",
    "master_df['clean_review'] = master_df[['reviews']].applymap(lambda x:indeed_review_cleanup(x))\n",
    "\n",
    "master_df['clean_review'].sort_values().unique()\n",
    "\n",
    "master_df['clean_review'] = master_df['clean_review'].astype(float)\n",
    "master_df['reviews'] = master_df['clean_review']\n",
    "master_df.drop('clean_review', axis=1, inplace=True)\n",
    "\n",
    "#########  END CLEAN REVIEWS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:45.392242",
     "start_time": "2016-10-17T17:59:45.187215"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### POST_DATE CLEAN TO FLOAT\n",
    "######\n",
    "\n",
    "try:\n",
    "    master_df['clean_post_date'] = master_df['post_date']\n",
    "except: pass\n",
    "\n",
    "\n",
    "def post_date_to_day_float(dateValue):\n",
    "    try:\n",
    "        temp = dateValue\n",
    "        dateValue.replace('s','')\n",
    "        if 'day' in dateValue:\n",
    "            temp = dateValue.split()[0]\n",
    "        elif 'hour' in dateValue:\n",
    "            temp = dateValue.split()[0]\n",
    "            temp = float(temp)/24\n",
    "        elif 'minute' in dateValue:\n",
    "            temp = dateValue.split()[0]\n",
    "            temp = float(temp)/24/60\n",
    "        if '+' in dateValue:\n",
    "            temp = 45           \n",
    "    except: \n",
    "        pass\n",
    "    return temp\n",
    "\n",
    "master_df['clean_post_date'] = master_df[['clean_post_date']].applymap(lambda x: post_date_to_day_float(x))\n",
    "\n",
    "master_df['clean_post_date'].sort_values().unique()\n",
    "\n",
    "master_df['clean_post_date'] = master_df['clean_post_date'].astype(float)\n",
    "master_df['post_date'] = master_df['clean_post_date']\n",
    "master_df.drop('clean_post_date', axis=1, inplace=True)\n",
    "master_df.rename(columns = {'post_date':'post_date_daysAgo'}, inplace=True)\n",
    "\n",
    "#########  END CLEAN POST_DATE\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:45.958279",
     "start_time": "2016-10-17T17:59:45.834855"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### STARS CLEAN TO FLOAT\n",
    "######\n",
    "\n",
    "\n",
    "master_df['clean_stars'] = master_df['stars'].fillna(0)\n",
    "master_df['clean_stars'] = master_df[['stars']].astype(float).applymap(lambda x: x//6/2)\n",
    "\n",
    "\n",
    "master_df['stars'] = master_df['clean_stars']\n",
    "master_df.drop('clean_stars', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#########  END CLEAN STARS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:46.346408",
     "start_time": "2016-10-17T17:59:46.306572"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####Create JOB_LINK column from JKID\n",
    "#####\n",
    "\n",
    "master_df['job_link'] = master_df[['jkid']].applymap(lambda x: 'http://www.indeed.com/rc/clk?jk='+x)\n",
    "\n",
    "#########  END JOB_LINK COLUMN\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:47.176608",
     "start_time": "2016-10-17T17:59:46.949854"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Location Cleanup\n",
    "#####\n",
    "\n",
    "def location_cleanup(location):\n",
    "    temp = location\n",
    "    temp_city = location.split(',')[0]\n",
    "    try:\n",
    "        temp_state = location.split(',')[1].split()[0]\n",
    "    except: \n",
    "        temp_state = ''\n",
    "    return temp_city+\", \"+temp_state\n",
    "    \n",
    "master_df['location_clean'] = master_df[['location']].applymap(lambda x: location_cleanup(x))\n",
    "master_df['location_clean'].sort_values().unique()\n",
    "\n",
    "master_df['location'] = master_df['location_clean']\n",
    "master_df.drop('location_clean', axis=1, inplace=True)\n",
    "\n",
    "#########  END LOCATION CLEANUP COLUMN\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:47.843245",
     "start_time": "2016-10-17T17:59:47.538130"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Salary Cleanup\n",
    "#####\n",
    "\n",
    "master_df['salary'] = master_df['salary'].fillna(0)\n",
    "\n",
    "def cleanup_salary(salary):\n",
    "    if \"year\" in str(salary):\n",
    "        temp = salary.strip(\" a year\")\n",
    "        temp = temp.split('-')\n",
    "        low_range = int(temp[0].strip().replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "        high_range = int(temp[-1].strip().replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "        avg = (low_range + high_range) / 2\n",
    "        salary_list = [low_range,high_range,avg]\n",
    "    elif \"month\" in str(salary):\n",
    "        temp = salary.replace(\"a month\",\"\")\n",
    "        temp = temp.split('-')\n",
    "        low_range = int(temp[0].replace(\"$\",\"\").replace(\",\",\"\"))*12\n",
    "        high_range = int(temp[-1].replace(\"$\",\"\").replace(\",\",\"\"))*12\n",
    "        avg = (low_range + high_range) / 2\n",
    "        salary_list = [low_range,high_range,avg]\n",
    "    elif \"hour\" in str(salary):\n",
    "        temp = salary.replace(\"an hour\",\"\")\n",
    "        temp = temp.split('-')\n",
    "        low_range = float(temp[0].replace(\"$\",\"\").replace(\",\",\"\"))*2080\n",
    "        high_range = float(temp[-1].replace(\"$\",\"\").replace(\",\",\"\"))*2080\n",
    "        avg = (low_range + high_range) / 2\n",
    "        salary_list = [low_range,high_range,avg]\n",
    "    else:\n",
    "        salary_list = [0,0,0]\n",
    "        low_range = 0\n",
    "        high_range = 0\n",
    "        avg = 0\n",
    "        \n",
    "    return low_range, high_range, avg\n",
    "\n",
    "master_df['salary_clean'] = master_df[['salary']].applymap(lambda x: cleanup_salary(x))\n",
    "\n",
    "master_df['salary'] = master_df['salary_clean']\n",
    "\n",
    "master_df['sal_low'] = master_df['salary'].apply(lambda x: x[0])\n",
    "master_df['sal_high'] = master_df['salary'].apply(lambda x: x[1])\n",
    "master_df['sal_avg'] = master_df['salary'].apply(lambda x: x[2])\n",
    "\n",
    "master_df.drop('salary_clean', axis=1, inplace=True)\n",
    "master_df.drop('salary', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#########  END SALARY CLEANUP COLUMN\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:48.610340",
     "start_time": "2016-10-17T17:59:48.517592"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = master_df.reset_index(drop=True)\n",
    "#Cleans title\n",
    "master_df['title'] = map(lambda x: x.lower(), master_df['title'])\n",
    "master_df['title'] = (master_df['title'].replace( 'sr.','senior ', regex=True))\n",
    "master_df['title'] = (master_df['title'].replace( 'senior  ','senior ', regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:49.098474",
     "start_time": "2016-10-17T17:59:49.040205"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove title from database\n",
    "temp_df = master_df.T\n",
    "temp_df = temp_df[[ind for ind, x in enumerate(master_df['title']) if \"title\" not in x]].T\n",
    "master_df = temp_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:49.521434",
     "start_time": "2016-10-17T17:59:49.476340"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove Intern from database\n",
    "temp_df = master_df.T\n",
    "temp_df = temp_df[[ind for ind, x in enumerate(master_df['title']) if \"intern\" not in x]].T\n",
    "master_df = temp_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:50.039321",
     "start_time": "2016-10-17T17:59:49.989118"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove Devop from database\n",
    "temp_df = master_df.T\n",
    "temp_df = temp_df[[ind for ind, x in enumerate(master_df['title']) if \"devop\" not in x]].T\n",
    "master_df = temp_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:59:50.967905",
     "start_time": "2016-10-17T17:59:50.961584"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_range = len(master_df)\n",
    "master_df['level'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:50:14.730947",
     "start_time": "2016-10-17T17:50:14.593839"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Entry = 0\n",
    "#Middle = 1\n",
    "#Senior = 2\n",
    "# for x in range(df_range):\n",
    "#     if \"associate\" in master_df['title'][x] or \"junior\" in master_df['title'][x] or \"entry\" in master_df['title'][x]:\n",
    "#         master_df['level'][x] = 0\n",
    "#     elif \"senior\" in master_df['title'][x] or \"principal\" in master_df['title'][x] or \"lead\" in master_df['title'][x]:\n",
    "#         master_df['level'][x] = 2\n",
    "#     else:  \n",
    "#         master_df['level'][x] = 1\n",
    "\n",
    "def bin_titles(title):\n",
    "    if any(title 'associate' in title:\n",
    "        return 0\n",
    "    elif 'junior' in title: \n",
    "        return 0\n",
    "    elif 'entry' in title: \n",
    "        return 0\n",
    "    elif 'senior' in title: \n",
    "        return 2\n",
    "    elif 'principal' in title: \n",
    "        return 2\n",
    "    elif 'lead' in title: \n",
    "        return 2\n",
    "    else: \n",
    "        return 1\n",
    "    \n",
    "        \n",
    "master_df['level'] = master_df[['title']].applymap(lambda x: bin_titles(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T17:52:47.243785",
     "start_time": "2016-10-17T17:52:47.234532"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9009\n",
       "2    2785\n",
       "0     975\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T18:40:21.093775",
     "start_time": "2016-10-17T18:40:21.055165"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n",
      "mean 104725.185663 median 97500.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_salary = master_df[master_df['sal_avg'] != 0]['sal_avg'].to_frame().mean()\n",
    "median_salary = master_df[master_df['sal_avg'] != 0]['sal_avg'].to_frame().median()\n",
    "\n",
    "print master_df[master_df['sal_avg'] != 0].shape[0]\n",
    "\n",
    "print 'mean {} median {}'.format(mean_salary[-1], median_salary[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:31:45.204786",
     "start_time": "2016-10-17T12:31:44.858859"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "has_salary = master_df[master_df['salary'] != (0,0,0)].shape[0]\n",
    "all_records = master_df.shape[0]\n",
    "print \"Job listings with salary info:\", has_salary\n",
    "print \"Total job listings: \", all_records\n",
    "print \"Salaried listings / Total listings\", round((float(has_salary) / all_records) * 100, 3), '%'\n",
    "master_df.head(5)\n",
    "master_df['title'].sort_values().unique()\n",
    "\n",
    "\n",
    "# data['recipe'] = data['title'].str.lower().str.contains('recipe')\n",
    "# data['electronic'] = data['title'].str.lower().str.contains('electronic')\n",
    "# data['tips'] = data['title'].str.lower().str.contains('tips')\n",
    "\n",
    "# stacked = pd.DataFrame(master_df['summary'].str.split().tolist()).stack()\n",
    "# final = pd.DataFrame(stacked.value_counts())\n",
    "# final.reset_index(inplace=True)\n",
    "# final['unique'] = final['index'].sort_values().unique()\n",
    "# final['unique']\n",
    "# import nltk\n",
    "# final['tagged'] = final[['index']].applymap(lambda x: nltk.pos_tag(x.strip()))\n",
    "# final.info()\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:26:13.238484",
     "start_time": "2016-10-17T13:26:12.403271"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer(\n",
    "    lowercase = True,\n",
    "    binary=True,  # Create binary features\n",
    "    stop_words='english', # Ignore common words such as 'the', 'and'\n",
    "    max_features=50, # Only use the top 50 most common words\n",
    ")\n",
    "\n",
    "\n",
    "# This builds a matrix with a row per website (or data point) and column per word (using all words in the dataset)\n",
    "X = v.fit_transform(master_df.summary).todense()\n",
    "X = pd.DataFrame(X, columns=v.get_feature_names())\n",
    "print X.columns.tolist()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:25:00.738562",
     "start_time": "2016-10-17T13:25:00.319132"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer(\n",
    "    lowercase = True, \n",
    "    binary=True,  # Create binary features\n",
    "    stop_words='english', # Ignore common words such as 'the', 'and'\n",
    "    max_features=50, # Only use the top 50 most common words\n",
    ")\n",
    "\n",
    "\n",
    "# This builds a matrix with a row per website (or data point) and column per word (using all words in the dataset)\n",
    "X = v.fit_transform(master_df.title).todense()\n",
    "X = pd.DataFrame(X, columns=v.get_feature_names())\n",
    "print X.columns.tolist()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:42:11.482253",
     "start_time": "2016-10-17T12:42:11.017815"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacked = pd.DataFrame(master_df['summary'].str.split().tolist()).stack()\n",
    "import string\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "final = pd.DataFrame(stacked.value_counts())\n",
    "print final.info()\n",
    "final.reset_index(inplace=True)\n",
    "final['unique'] = final['index'].sort_values().unique()\n",
    "final['unique']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:30:59.512155",
     "start_time": "2016-10-17T12:30:59.483438"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-16T23:22:22.556939",
     "start_time": "2016-10-16T23:22:22.552847"
    },
    "collapsed": false,
    "focus": false,
    "id": "6e259594-1c52-436b-ab9e-527e071941c1"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "1. Some of the salaries are not yearly but hourly, these will be useful to us for now\n",
    "2. The salaries are given as text and usually with ranges.\n",
    "\n",
    "#### Filter out the salaries that are not yearly (filter those that refer to hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "58533e57-f86b-494a-b841-e7b59c6229c6"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "7d4bc860-b214-4f75-9cd0-b234830b1ec2"
   },
   "source": [
    "#### Write a function that takes a salary string and converts it to a number, averaging a salary range if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "a0f701e0-80bd-40ba-9101-4535860c0968"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
